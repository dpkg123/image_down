#!/bin/bash
# A shell script to download random images from https://iw233.cn/API/Random.php
# Usage: ./image_down <folder> <number>

set -e

# Check if the folder and number arguments are provided
if [ $# -ne 2 ]; then
  echo "Usage: ./image_down <folder> <number>"
  exit 1
fi

A=$(date +%s)

concurrent_downloads=16 # 你可以调整这个值来设置并发下载数

folder=$1
number=$2

if [ ! -d "$folder" ]; then
  mkdir -p "$folder"
fi

# 定义待测试的站点列表
urls=(
  "https://dev.iw233.cn/api.php?sort=random"
  "https://api.iw233.cn/api.php?sort=random"
  "https://iw233.cn/api.php?sort=random"
)

# 初始化最快站点和时间
fastest_url=""
min_time=999999

# 检测每个站点
for url in "${urls[@]}"; do
  # 使用curl测试连接时间
  time=$(curl -o /dev/null -s -w '%{time_connect}' --connect-timeout 5 "$url")
  http_code=$(curl -o /dev/null -s -w "%{http_code}" "$url")

  # 检查http状态码是否为403
  if [ "$http_code" -eq 403 ]; then
    echo "Site $url is forbidden (HTTP 403). Skipping."
    continue
  fi

  # 检查是否找到更快的站点
  if [ $(awk 'BEGIN{print "'$time'" < "'$min_time'"}') -eq 1 ]; then
      min_time=$time
    fastest_url=$url
  fi
done

# 检查是否找到可用站点
if [ -z "$fastest_url" ]; then
  echo "No available site found. Exiting."
  exit 1
fi

echo "Using the fastest site: $fastest_url"

for ((i=1; i<=number; i++)); do
  (
    retry_count=0
    max_count=3
    while [ $retry_count -lt $max_retries ]; do
      filename=$(date +%s%N).jpg
      if curl -sL "$fastest_url" -o "$folder/$filename"; then
        echo "Downloaded image $i of $number to $folder/$filename"
        break
      else
        echo "Failed to download image $i. Retrying..."
        ((retry_count++))
      fi
    done
  ) &
  
  # 如果后台进程数达到$concurrent_downloads，就等待任一进程结束
  if (( $(jobs -p | wc -l) >= concurrent_downloads )); then
    wait -n
  fi
done

# 等待所有后台进程结束
wait

B=$(date +%s)
C=$(expr $B - $A)

echo "Done. Downloaded $number images to $folder, use to $(expr $C / 60)min$(expr $C % 60)s."
