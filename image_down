#!/bin/bash
# A shell script to download random images from https://iw233.cn/API/Random.php
# Usage: ./image_down <folder> <number>

set -e

# Check if the folder and number arguments are provided
if [ $# -ne 2 ]; then
  echo "Usage: ./image_down <folder> <number>"
  exit 1
fi

A=$(date +%s)

concurrent_downloads=100 # 你可以调整这个值来设置并发下载数

folder=$1
number=$2

# 初始化最快站点和时间
fastest_url=""
min_time=999999

# 普通浏览器
#UA='Mozilla/5.0 (Windows NT 6.4; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.2135.0 Safari/537.36'
# 百度蜘蛛
#UA="Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
# PS3
#UA="Mozilla/5.0 (PLAYSTATION 3 4.90) AppleWebKit/531.22.8 (KHTML, like Gecko)"
# Linux
UA='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.2903.9'
# IE11
#UA="Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; LCTE; rv:11.0) like Gecko"

if [ ! -d "$folder" ]; then
  mkdir -p "$folder"
fi

# 定义待测试的站点列表
urls=(
  "https://cnmiw.com/api.php?sort=setu"
  "https://cnmiw.com/api.php?sort=top"
  "https://cnmiw.com/api.php?sort=random"
  #"https://api.iw233.cn/api.php?sort=random"
  #"https://iw233.cn/api.php?sort=random"
)

export UA fastest_url folder number

# 创建临时目录
TMPDIR=$(mktemp -d)
trap 'rm -rf "$TMPDIR"' EXIT

# 并行探测所有 URL
printf '%s\n' "${urls[@]}" | parallel -j+0 --line-buffer '
    url=$(echo "$1" | sed "s/[[:space:]]*$//")  # 去掉末尾空格（如 URL 含空格）
    if [ -z "$url" ]; then exit 1; fi

    # 一次 curl 获取 HTTP 状态码和连接时间
    result=$(curl -H "User-Agent: $UA" \
                 --referer "https://www.baidu.com/s?wd=iw233" \
                 -o /dev/null \
                 -s -w "%{http_code} %{time_connect}" \
                 --connect-timeout 5 \
                 --max-time 8 \
                 "$url" 2>/dev/null)

    http_code=$(echo "$result" | awk "{print \$1}")
    time_connect=$(echo "$result" | awk "{print \$2}")

    # 过滤无效响应：403、0（连接失败）、非 2xx/3xx
    if [ "$http_code" -eq 403 ] || [ "$http_code" -eq 0 ] || [ "$http_code" -lt 200 ] || [ "$http_code" -ge 500 ]; then
        exit 1
    fi

    # 输出格式: <time_connect> <url>
    echo "$time_connect $url"
' > "$TMPDIR/fastest_candidates.txt"

# 选出最快（时间最小）的 URL
if [ -s "$TMPDIR/fastest_candidates_txt" ]; then
    fastest_url=$(sort -n "$TMPDIR/fastest_candidates.txt" | head -n1 | cut -d' ' -f2-)
else
    fastest_url=""
fi

# 清理（trap 会自动删 TMPDIR，此处可省略）
rm -rf "$TMPDIR"

# 检查结果
if [ -z "$fastest_url" ]; then
    echo "No available site found. Exiting." >&2
    exit 1
fi

echo "Using the fastest site: $fastest_url"

seq 1 "$number" | parallel -j "$concurrent_downloads" --line-buffer \
  'filename="$folder/$(date +%s%N).jpg"; \
   if curl -H "User-Agent: $UA" \
          --referer "https://weibo.com/ " \
          -H "Accept-Language: zh-CN,cn;q=0.9" \
          --fail -sS -L \
          "$fastest_url" -o "$filename"; then \
       echo "Downloaded {} of $number. filename: $filename"; \
   else \
       echo "Failed to download image {}" >&2; \
   fi'

B=$(date +%s)
C=$(expr $B - $A)

echo "Done. Downloaded $number images to $folder, use to $(expr $C / 60)min$(expr $C % 60)s."
