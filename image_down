#!/bin/bash
# A shell script to download random images from https://iw233.cn/API/Random.php
# Usage: ./image_down <folder> <number>

set -e

# Check if the folder and number arguments are provided
if [ $# -ne 2 ]; then
  echo "Usage: ./image_down <folder> <number>"
  exit 1
fi

A=$(date +%s)

concurrent_downloads=25 # 你可以调整这个值来设置并发下载数

folder=$1
number=$2

# 初始化最快站点和时间
fastest_url=""
min_time=999999

# 普通浏览器
UA='Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1667.0 Safari/537.36'
# 百度蜘蛛
#UA="Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
#UA="Mozilla/5.0 (PLAYSTATION 3 4.90) AppleWebKit/531.22.8 (KHTML, like Gecko)"

if [ ! -d "$folder" ]; then
  mkdir -p "$folder"
fi

# 定义待测试的站点列表
urls=(
  "https://dev.iw233.cn/api.php?sort=random"
  "https://api.iw233.cn/api.php?sort=random"
  "https://iw233.cn/api.php?sort=random"
)

# 检测每个站点
for url in "${urls[@]}"; do
  # 使用curl测试连接时间
  time=$(curl -H "User-Agent: $UA" --referer https://www.baidu.com/s?wd=iw233 -o /dev/null -s -w '%{time_connect}' --connect-timeout 5 "$url")
  http_code=$(curl -o /dev/null -H "User-Agent: $UA" --referer https://www.baidu.com/s?wd=iw233 -s -w "%{http_code}" "$url")

  # 检查http状态码是否为403
  if [ "$http_code" -eq 403 ]; then
    echo "Site $url is forbidden (HTTP 403). Skipping."
    continue
  fi

  # 检查是否找到更快的站点
  if [ $(awk 'BEGIN{print "'$time'" < "'$min_time'"}') -eq 1 ]; then
      min_time=$time
    fastest_url=$url
  fi
done

# 检查是否找到可用站点
if [ -z "$fastest_url" ]; then
  echo "No available site found. Exiting."
  exit 1
fi

echo "Using the fastest site: $fastest_url"

# 生成下载任务并使用 xargs 并发执行
seq 1 $number | parallel -j $concurrent_downloads "curl  -H "User-Agent: '$UA'" --referer https://www.baidu.com/s?wd=iw233 -sL $fastest_url -o '$folder/{}.jpg' && echo 'Downloaded image {}'"

B=$(date +%s)
C=$(expr $B - $A)

echo "Done. Downloaded $number images to $folder, use to $(expr $C / 60)min$(expr $C % 60)s."
