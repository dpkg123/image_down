#!/bin/bash
# A shell script to download random images from https://iw233.cn/API/Random.php
# Usage: ./image_down <folder> <number>

set -e

# Check if the folder and number arguments are provided
if [ $# -ne 2 ]; then
  echo "Usage: ./image_down <folder> <number>"
  exit 1
fi

A=$(date +%s)

concurrent_downloads=100 # 你可以调整这个值来设置并发下载数

folder=$1
number=$2

# 初始化最快站点和时间
fastest_url=""
min_time=999999

# 普通浏览器
#UA='Mozilla/5.0 (Windows NT 6.4; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.2135.0 Safari/537.36'
# 百度蜘蛛
#UA="Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
# PS3
#UA="Mozilla/5.0 (PLAYSTATION 3 4.90) AppleWebKit/531.22.8 (KHTML, like Gecko)"
# Linux
UA='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.2903.9'
# IE11
#UA="Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; LCTE; rv:11.0) like Gecko"

if [ ! -d "$folder" ]; then
  mkdir -p "$folder"
fi

# 定义待测试的站点列表
urls=(
  "https://cnmiw.com/api.php?sort=setu"
  "https://cnmiw.com/api.php?sort=top"
  "https://cnmiw.com/api.php?sort=random"
  #"https://api.iw233.cn/api.php?sort=random"
  #"https://iw233.cn/api.php?sort=random"
)

export UA fastest_url folder number

# 并行探测（兼容所有 POSIX shell）
TMPDIR=$(mktemp -d)
trap 'rm -rf "$TMPDIR"' EXIT

i=0
for url in "${urls[@]}"; do
    # 去掉 URL 末尾空格
    url=$(printf '%s' "$url" | sed 's/[[:space:]]*$//')
    [ -z "$url" ] && continue

    (
        result=$(curl -H "User-Agent: $UA" \
                     --referer "https://www.baidu.com/s?wd=iw233" \
                     -o /dev/null -s -w "%{http_code} %{time_connect}" \
                     --connect-timeout 5 --max-time 8 "$url" 2>/dev/null) || exit 1

        http_code=$(echo "$result" | awk '{print $1}')
        time_connect=$(echo "$result" | awk '{print $2}')

        if [ "$http_code" -eq 403 ] || [ "$http_code" -eq 0 ] || [ "$http_code" -lt 200 ] || [ "$http_code" -ge 500 ]; then
            exit 1
        fi

        printf "%s %s\n" "$time_connect" "$url"
    ) > "$TMPDIR/result.$((i++))" &
done

# 等待所有探测完成
wait

# 合并结果
cat "$TMPDIR"/result.* 2>/dev/null | sort -n | head -n1 | cut -d' ' -f2- > "$TMPDIR/fastest"

fastest_url=$(cat "$TMPDIR/fastest")

if [ -z "$fastest_url" ]; then
    echo "No available site found. Exiting." >&2
    exit 1
fi

echo "Using the fastest site: $fastest_url"

seq 1 "$number" | parallel -j "$concurrent_downloads" --line-buffer \
  'filename="$folder/$(date +%s%N).jpg"; \
   if curl -H "User-Agent: $UA" \
          --referer "https://weibo.com/ " \
          -H "Accept-Language: zh-CN,cn;q=0.9" \
          --fail -sS -L \
          "$fastest_url" -o "$filename"; then \
       echo "Downloaded {} of $number. filename: $filename"; \
   else \
       echo "Failed to download image {}" >&2; \
   fi'

B=$(date +%s)
C=$(expr $B - $A)

echo "Done. Downloaded $number images to $folder, use to $(expr $C / 60)min$(expr $C % 60)s."
